# Вопрос 11. Метод Ньютона минимизации функций многих переменных.

**Где читать в учебнике:**
*   **Стр. 116:** Раздел 3.6.4.
*   **Стр. 117:** Формулы (3.69), (3.70).

**Суть:** Это самый мощный метод безусловной оптимизации. Он использует информацию о кривизне поверхности (вторые производные), чтобы прыгнуть сразу в "центр ямы".

---

## 1. Идея метода (На пальцах)

> [!example] Параболическая чаша
> В методе градиентного спуска (Q10) мы аппроксимировали функцию **плоскостью** (касательной) и шли вниз.
> В методе Ньютона мы аппроксимируем функцию **параболоидом** (квадратичной чашей).
>
> 1.  Стоим в точке $x_k$.
> 2.  Строим вокруг себя "виртуальную чашу" (на основе первых и вторых производных).
> 3.  Прыгаем сразу в **дно** этой виртуальной чаши.
>
> Если реальная функция сама по себе квадратичная (как $x^2 + y^2$), метод находит ответ **за 1 шаг** из любой точки!

---

## 2. Математическая формулировка

Мы раскладываем функцию в ряд Тейлора до второго порядка:
$$ f(x) \approx f(x_k) + \langle f'(x_k), x - x_k \rangle + \frac{1}{2} \langle f''(x_k)(x - x_k), x - x_k \rangle $$

Чтобы найти минимум этой "чаши", нужно, чтобы градиент стал равен нулю. Это приводит к формуле шага.

> [!info] Алгоритм метода Ньютона (Полный цикл)
>
> **Входные данные:** Начальная точка $x^0$, точность $\varepsilon$.
> Положить счетчик итераций $k = 0$.
>
> **Шаг 1. Вычисление градиента.**
> В текущей точке $x^k$ вычислить вектор градиента $\nabla f(x^k)$.
>
> **Шаг 2. Проверка критерия остановки.**
> Вычислить норму (длину) градиента $\|\nabla f(x^k)\|$.
> *   **Если** $\|\nabla f(x^k)\| < \varepsilon$, то **СТОП**.
>     Текущая точка $x^k$ является приближенным решением ($x^* \approx x^k$).
> *   **Иначе** (если градиент большой) — идем к Шагу 3.
>
> **Шаг 3. Вычисление Гессиана.**
> Вычислить матрицу вторых производных $H(x^k) = f''(x^k)$ и найти обратную матрицу $[H(x^k)]^{-1}$.
>
> **Шаг 4. Спуск (Итерация).**
> Вычислить новую точку:
> $$ x^{k+1} = x^k - [f''(x^k)]^{-1} \cdot \nabla f(x^k) $$
>
> **Шаг 5. Зацикливание.**
> Увеличить счетчик: $k = k + 1$.
> **Перейти к Шагу 1.**



---

## 3. Плюсы и минусы (Важно для экзамена)

| Плюсы (+) | Минусы (-) |
| :--- | :--- |
| **Скорость:** Сходится мгновенно (квадратичная скорость), если мы рядом с минимумом. | **Сложность:** Нужно считать $n^2$ вторых производных и обращать огромную матрицу на каждом шаге. |
| **Точность:** Очень высокая. | **Ненадежность:** Если начать далеко от минимума, метод может "улететь" в бесконечность. Работает только для выпуклых функций. |

*Примечание:* Чтобы метод не ломался, часто вводят регулировку шага $\alpha_k$, как в градиентном спуске: $x^{k+1} = x^k - \alpha_k [f'']^{-1} f'$.

---

#### К пункту "3. Плюсы и минусы"

> [!info] Плюсы и минусы
> | Плюсы (+) | Минусы (-) |
> | :--- | :--- |
> | **Скорость:** Квадратичная сходимость. | **Сложность:** <br> 1. Нужно считать $n$ первых производных.<br> 2. Нужно считать $n^2$ **вторых** производных.<br> 3. Нужно на каждой итерации **обращать матрицу $n \times n$** (это $O(n^3)$ операций).<br> 4. Требуется, чтобы Гессиан был **положительно определён** (иначе можно попасть не в минимум, а в максимум или седло). |
> | **Точность:** Очень высокая. | **Ненадежность:** Если начать далеко от минимума, метод может "улететь" в бесконечность. |

---

### 3. Пример (Как это выглядит на деле)

**Задача (из учебника, с. 112, Пример 3.11):**
Найти минимум $f(x_1, x_2) = x_1^2 + 100x_2^2$.

1.  **Градиент ($f'$):**
    $$ \nabla f = \begin{pmatrix} 2x_1 \\ 200x_2 \end{pmatrix} $$

2.  **Матрица Гессе ($f''$):**
    $$ H = \begin{pmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} \end{pmatrix} = \begin{pmatrix} 2 & 0 \\ 0 & 200 \end{pmatrix} $$

3.  **Обратная матрица Гессе ($H^{-1}$):**
    $$ H^{-1} = \begin{pmatrix} 1/2 & 0 \\ 0 & 1/200 \end{pmatrix} $$

4.  **Шаг Ньютона:**
    $$ \begin{pmatrix} x_1^{k+1} \\ x_2^{k+1} \end{pmatrix} = \begin{pmatrix} x_1^k \\ x_2^k \end{pmatrix} - \begin{pmatrix} 1/2 & 0 \\ 0 & 1/200 \end{pmatrix} \begin{pmatrix} 2x_1^k \\ 200x_2^k \end{pmatrix} $$
    $$ = \begin{pmatrix} x_1^k \\ x_2^k \end{pmatrix} - \begin{pmatrix} x_1^k \\ x_2^k \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} $$
В этом примере (квадратичная функция) метод Ньютона находит оптимум $(0,0)$ **за один шаг** из любой начальной точки.

---

## 4. Навигация
<< [[Q10 - Градиентный спуск]] | [[00_ПЛАН_ПОДГОТОВКИ]] | [[Q12 - Выпуклое программирование]] >>